{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3f95897-b61d-49c2-8c44-db657d4841e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00bbede8-a9cb-4c89-a2df-3b61f90ddf48",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_df = pd.read_csv(r'train.csv')\n",
    "test_df = pd.read_csv(r'test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ac09f3c-1db7-40f8-b812-5d3a0f4a5bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bceb6ef-5819-401d-97af-7a8288db30d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "437a01f9-4be4-4ecc-9407-5c72fffc48da",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"First 5 rows of the training data:\")\n",
    "train_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d3a570a-ac98-4545-98c7-29c0faec6637",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"\\nFirst 5 rows of the test data:\")\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c222273c-23f5-4944-b8a3-dd3f20e93a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a34b57cf-8c9e-48d5-b5fe-1b4450fd0f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d618d050-d97e-4d1e-b672-bd6885733768",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"\\nTraining data information:\")\n",
    "print(train_df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0509e076-c13a-4140-89cb-435020e1f608",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"\\nTest data information:\")\n",
    "print(test_df.info())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd4ab2d2-ae0a-48d0-a8e1-74762e37ceee",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.duplicated().sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e105f3ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c290dcc-2d72-4778-bfa5-5c256741ae96",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.duplicated().sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc24a685-39e5-4142-b9f0-8a6bf575c505",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[\"Address\"].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78d1b32c-eb25-4b91-885e-7cfe92d4c951",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['Dates'] = pd.to_datetime(train_df['Dates'])\n",
    "\n",
    "test_df['Dates'] = pd.to_datetime(test_df['Dates'])\n",
    "\n",
    "(train_df[['Dates']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc6514ae-dc16-4eea-8679-b02586de5d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "sf_geobounds = {\n",
    "    'min_lat': 37.70,  \n",
    "    'max_lat': 37.84,   \n",
    "    'min_lon': -122.52, \n",
    "    'max_lon': -122.35\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7947cba-a14b-4e41-8d1e-9b5849ac894b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_df['is_in_sf'] = np.where(\n",
    "    (train_df['Y'].between(sf_geobounds['min_lat'], sf_geobounds['max_lat'])) & \n",
    "    (train_df['X'].between(sf_geobounds['min_lon'], sf_geobounds['max_lon'])),\n",
    "    True,  \n",
    "    False  \n",
    ")\n",
    "\n",
    "print(f\"Number of crimes outside the boundaries: {len(train_df[train_df['is_in_sf'] == False])}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3325af76-a978-4cec-9b10-3791777f704c",
   "metadata": {},
   "outputs": [],
   "source": [
    "suspicious_crimes = train_df[train_df['is_in_sf'] == False]\n",
    "print(suspicious_crimes['Category'].value_counts().head(5))\n",
    "\n",
    "ocean_crimes = suspicious_crimes[(suspicious_crimes['Y'] < 37.70) | (suspicious_crimes['X'] < -122.52)]\n",
    "print(f\"Crimes in the ocean: {len(ocean_crimes)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "effe6c61-dd03-4ebe-97fa-84bde8f47ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean = train_df[train_df['is_in_sf'] == True].copy()\n",
    "\n",
    "print(f\"Original data: {len(train_df)} records\")\n",
    "print(f\"After cleaning: {len(df_clean)} records\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b99aba05-1b93-4a49-8070-c0a0715436c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean['Year'] = df_clean['Dates'].dt.year\n",
    "df_clean['Month'] = df_clean['Dates'].dt.month\n",
    "df_clean['Day'] = df_clean['Dates'].dt.day\n",
    "df_clean['Hour'] = df_clean['Dates'].dt.hour\n",
    "df_clean['DayOfWeek'] = df_clean['Dates'].dt.day_name() \n",
    "\n",
    "print(df_clean[['Dates', 'Year', 'Month', 'Day', 'Hour', 'DayOfWeek']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efce6d26-ab11-4af6-8816-d6f0a143562a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le = LabelEncoder()\n",
    "\n",
    "df_clean['Category_encoded'] = le.fit_transform(df_clean['Category'])\n",
    "\n",
    "print(df_clean[['Category', 'Category_encoded']].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aedfffc-a7cd-4477-96e6-104ad088ec89",
   "metadata": {},
   "outputs": [],
   "source": [
    "district_encoded = pd.get_dummies(df_clean['PdDistrict'], prefix='District')\n",
    "\n",
    "df_clean = pd.concat([df_clean, district_encoded], axis=1)\n",
    "\n",
    "print(df_clean.filter(regex='District_').head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e30c89-4d0c-4f71-8f23-34e32259bfd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean[\"Category\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a2367e2-ce0c-4c2b-a3ce-f517182ec6b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean[\"Resolution\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6971a04e-ad27-4e40-b25f-48c267781989",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean[\"DayOfWeek\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e45631e5-6b6f-4322-870f-71e31ffecf66",
   "metadata": {},
   "source": [
    "# Hierarchical Cluster Algorithm is supported by the Genetic Algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80573ed7-ca3c-4909-83dd-aba1edf3275a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import my libraries\n",
    "from sklearn.cluster import AgglomerativeClustering # to make cluster\n",
    "from sklearn.metrics import silhouette_score # to evaluate this cluster\n",
    "from sklearn.preprocessing import StandardScaler # to normalize the data\n",
    "import random # to choose random values\n",
    "from scipy.cluster.hierarchy import dendrogram,linkage,fcluster # to make cluster ,draw,know labels\n",
    "import matplotlib.pyplot as plt # to draw "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "894eb00a-9fd4-406a-b272-11b7bbab39a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I will choose the numerical feature that i will use to cluster the data based on them (but we don't know those catogeries(unsupervised learning))\n",
    "# we will normalize two columns by using standerscaler\n",
    "data=df_clean[['X','Y']]\n",
    "scalarx=StandardScaler()\n",
    "scalary=StandardScaler()\n",
    "x=np.array(data['X'])# I change them into array to can easily work with them (series are difficult)\n",
    "y=np.array(data['Y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a957f54a-bff1-4655-8e58-5ff77f6c2543",
   "metadata": {},
   "outputs": [],
   "source": [
    "x=scalarx.fit_transform(x.reshape(-1,1)).flatten()# I use reshape to make it 2D \n",
    "y=scalarx.fit_transform(y.reshape(-1,1)).flatten()# flatten() to return it to 1D\n",
    "newdata=pd.DataFrame({'X':x,'Y':y})\n",
    "newdata=newdata.head(10000)# I choose only 10000 because the data is very large \n",
    "#but this model can't deal with large data well (he needs large space (2t))\n",
    "newdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e72b165e-ffe8-46fd-8a05-54d331de74e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zero generation of population has four parents with different data (different distance metric,different linkage criterion, threshold (k))\n",
    "# we use genatic algorthim to choose the best values for those parameters\n",
    "population={\n",
    "'parent1':[2,'ward','euclidean'],\n",
    "'parent2':[2,'single','manhattan'],\n",
    "'parent3':[2,'complete','mahalanobis'],\n",
    "'parent4':[2,'average','euclidean']}\n",
    "population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b24a781-966d-4b30-8fc5-c75385318a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will use fit function to choose the best 2 parents\n",
    "# first we will fit model to each parent then we will evaluate by using silhouette_score\n",
    "# we store each score ,then we ordered them to return the best 2 parents (with high si )\n",
    "def fitness(population,data):\n",
    "    totalscore={}\n",
    "    for i in population.keys():\n",
    "        model=AgglomerativeClustering(n_clusters=population[i][0],linkage=population[i][1],metric=population[i][2])\n",
    "        labels=model.fit_predict(data)\n",
    "        score=silhouette_score(data,labels)\n",
    "        totalscore[i]=[population[i],score]\n",
    "    totalscore=dict(sorted(totalscore.items(),key=lambda item: item[1][1],reverse=True))\n",
    "   \n",
    "    return dict(list(totalscore.items())[0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8837a34d-b321-43f5-983a-dcd382427bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# after we choose 2 parents we will do crossover between them and produce 2 new children \n",
    "def crossover(parents):\n",
    "    newgeneration={}\n",
    "    # new populattion has 2 best parents and 2 new children( we will use 2 old parents becasue they may be better than new children)\n",
    "    parent1data=list(parents.values())[1][0]#[5, 'average', 'euclidean'] this line will return from this step (in the same form)\n",
    "    parent2data=list(parents.values())[0][0]\n",
    "    newgeneration['parent1']=parent1data\n",
    "    newgeneration['parent2']=parent2data\n",
    "    # we will create to children , we will cut in postion number 1 (from linkage),we can choose another position or do it randomly\n",
    "    newgeneration['child1']=[parent1data[0],parent2data[1],parent2data[2]]\n",
    "    newgeneration['child2']=[parent2data[0],parent1data[1],parent1data[2]]\n",
    "    return newgeneration "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f2b7c35-4b3e-4c54-99ca-4389e3cfc305",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we will make mutation to change the value of k \n",
    "# we will choose one solution randomly \n",
    "# then we will change k value we will increase it by adding 1\n",
    "random.seed(42) # to choose the same position if we rerun the code.(unless : we will have different parameters at all time we run the code)\n",
    "np.random.seed(42)\n",
    "def mutation(newgeneration):\n",
    "    randomsolutionposition=random.randint(0,3)\n",
    "    list(newgeneration.items())[randomsolutionposition][1][0]+=1 # I arrive to k and add one\n",
    "    return newgeneration\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58137da9-1f18-46dc-98ea-56d570de4bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we will do this function to fit the best model we will use recursive to arrive to the best model.\n",
    "# we will stop after 5 iterations or more (as you like)\n",
    "#we will use  k to can stop when we complete 5 iterations\n",
    "# first we will take our population to calculate the fitness to all 4 solution.\n",
    "# then we will take the 2 best parents (return from fitness function) to generate new childrens\n",
    "# we will make all those steps to nextgeneration again until we finish our 5 iterations\n",
    "# when we finish our iterations we will fitness the nextgeneration to choose the final best one soluation (parameters)\n",
    "def choosebestmodel(population,data,k):\n",
    "    if (k==5): \n",
    "        bestsolution=fitness(population,data)\n",
    "        print(list(bestsolution.values())[0])\n",
    "        return # stop condition \n",
    "        \n",
    "    parents=fitness(population,data)\n",
    "    newgeneration=crossover(parents)\n",
    "    nextgeneration=mutation(newgeneration)\n",
    "    \n",
    "    # recursive\n",
    "    choosebestmodel(nextgeneration,data,k=k+1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b04d1317-70f4-4013-9431-610d4bb805d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# those parameters are very good ( silhouette_score is near from 1, this means a good model (cohesion)(separation) :-) )\n",
    "bestsolution=choosebestmodel(population,newdata,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3a07e3a-4111-4434-81d3-82d82a5db2c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# After we know the best parameters . we can fit model direct using them and draw the dendrogram.\n",
    "#(Now i will use linkaage with the good parameters that i get them from GA )\n",
    "bestmodel=linkage(newdata,method='average',metric='euclidean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dd25aad-d038-4628-8ffb-02ceafa6eb3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I will calculate the labels ( clusters ) .I will use t=2 ( best parameter from GA) ,\n",
    "# maxclust means i want the data divide into only 2 clusters\n",
    "# we will calculate the silhouette_score\n",
    "# silhouette_score are similar at two algorithm \n",
    "labels=fcluster(bestmodel,t=2,criterion='maxclust')\n",
    "score=score=silhouette_score(newdata,labels)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30764689-3e85-4442-a49c-875bf561da34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I draw dendrogram and line divide the data into 2 groups\n",
    "plt.figure(figsize=(15,7))\n",
    "dendrogram(bestmodel,no_labels=True)\n",
    "plt.axhline(y=.8,color='hotpink',linestyle='--')\n",
    "plt.title(\" Best Hierarchical cluster\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a8f7282",
   "metadata": {},
   "source": [
    "#  Random forest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34c746fe",
   "metadata": {},
   "source": [
    "First, we used Logistic Regression and Random Forest without any feature selection,\n",
    "and the accuracy was very low. \n",
    "After applying feature selection, \n",
    "the accuracy improved significantly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d69b375f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# i convert categorical data into numeric labels to use in my Model\n",
    "df_clean[\"PdDistrict_Label\"] = le.fit_transform(df_clean[\"PdDistrict\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd54cd61",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "# I will use RandomForestClassifier to predict the crime PdDistrict\n",
    "\n",
    "y = df_clean[\"PdDistrict_Label\"]\n",
    "x = df_clean[[\"X\", \"Y\"]]\n",
    "\n",
    "\n",
    "\n",
    "random_forest_model = RandomForestClassifier(\n",
    "    n_estimators=100,# Number of trees in the forest\n",
    "    max_depth=32,# Maximum depth of the tree\n",
    "    random_state=1# Random seed for reproducibility\n",
    ")\n",
    "# Fit the model to the data\n",
    "random_forest_model.fit(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b19add02",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df[\"PdDistrict_encoded\"] = le.transform(test_df[\"PdDistrict\"])# Convert categorical data into numeric labels\n",
    "y_test = test_df[\"PdDistrict_encoded\"]\n",
    "X_test= test_df[[\"X\", \"Y\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16980168",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_acc = random_forest_model.score(x,y)\n",
    "test_acc = random_forest_model.score(X_test, y_test)\n",
    "print(f\"Train Accuracy: {train_acc:.4f}\")\n",
    "print(f\"Test Accuracy: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec1ccb91",
   "metadata": {},
   "source": [
    "Here, we used a heatmap to find the features most correlated with our target variable Category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "184c2bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    " \n",
    "df_clean[\"District\"] = le.fit_transform(df_clean[\"Descript\"])\n",
    "\n",
    "df=df_clean[[\"X\",\"Y\", \"Year\", \"Month\", \"Day\", \"Hour\", \"PdDistrict_Label\", \"Holiday\", \"Weekend\", \"Category_encoded\", \"District\"]]\n",
    "corMatrix = df.corr()\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(corMatrix, annot=True, cmap=\"coolwarm\", fmt=\".2f\", square=True, cbar_kws={\"shrink\": .8})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b42569fd",
   "metadata": {},
   "source": [
    "we find that the correlation between \"X\", \"Y\", \"PdDistrict_Label\",\"Descript_Label\"and our target \"Category\" is very high"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d64ba350",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df_clean[\"Descript_Label\"] = le.fit_transform(df_clean[\"Descript\"])\n",
    "y=df_clean[\"Category_encoded\"]\n",
    "x=df_clean[[\"X\", \"Y\", \"PdDistrict_Label\",\"Descript_Label\"]]\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, stratify=y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba23dafc-b589-4aa8-9b3a-5bc834df3719",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "random_forest_modelcategory = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=32,\n",
    "    random_state=1\n",
    ")\n",
    "\n",
    "random_forest_modelcategory.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ab94346-ffa8-40cb-bdb5-0c9faf50ffe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_acc = random_forest_modelcategory.score(x,y)\n",
    "test_acc = random_forest_modelcategory.score(X_test, y_test)\n",
    "print(f\"Train Accuracy: {train_acc:.4f}\")\n",
    "print(f\"Test Accuracy: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd5f82b3",
   "metadata": {},
   "source": [
    "# KMedoids method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50ffe232",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import numpy as np\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "from tabulate import tabulate               # Import tabulate for creating formatted tables\n",
    "import seaborn as sns\n",
    "import plotly.express as px    # For interactive visualizations\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn_extra.cluster import KMedoids             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8532548d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "#Takes a random sample of 10,000 points (reduced from 20,000 for faster computation)\n",
    "\n",
    "#Uses numpy's random.choice to select indices without replacement\n",
    "\n",
    "coords = train_df[(train_df['X'].between(-123, -122)) & \n",
    "                 (train_df['Y'].between(37.7, 38))][['X', 'Y']].copy()\n",
    "sample_size = 10000  \n",
    "sample_indices = np.random.choice(coords.shape[0], sample_size, replace=False)\n",
    "sample_coords = coords.iloc[sample_indices]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "sample_scaled = scaler.fit_transform(sample_coords)\n",
    "# Initializes storage for silhouette scores\n",
    "silhouette_scores = []\n",
    "#Tests only k=2,3,4,5 clusters\n",
    "k_values = [2, 3, 4, 5] \n",
    "\n",
    "#Creates KMedoids instance for each k with:\n",
    "for k in k_values:\n",
    "    try:\n",
    "        # Use faster initialization and smaller max_iter\n",
    "        kmedoids = KMedoids(n_clusters=k, \n",
    "                           #k-medoids++ initialization (smarter than random)\n",
    "                           init='k-medoids++',\n",
    "                           #Fixed random state for reproducibility\n",
    "                           random_state=42,\n",
    "                           #Euclidean distance metric\n",
    "                           metric='euclidean',\n",
    "                           #Limited to 50 iterations for speed\n",
    "                           max_iter=50)  \n",
    "        #Performs clustering and gets labels\n",
    "        labels = kmedoids.fit_predict(sample_scaled)\n",
    "        #Checks if at least 2 clusters were formed ---> required for silhouette score\n",
    "\n",
    "        if len(np.unique(labels)) < 2:\n",
    "            print(f\"Skipping k={k} (only 1 cluster formed)\")\n",
    "            continue\n",
    "        #Calculates silhouette score (measures cluster separation)\n",
    "        #Stores and prints the score for each k    \n",
    "        score = silhouette_score(sample_scaled, labels)\n",
    "        silhouette_scores.append(score)\n",
    "        print(f\"k={k}: Silhouette Score = {score:.3f}\")\n",
    "        #To handle with errors\n",
    "    except Exception as e:\n",
    "        print(f\"Error with k={k}: {str(e)}\")\n",
    "        continue\n",
    "\n",
    "# Determine optimal k\n",
    "if not silhouette_scores:\n",
    "    raise ValueError(\"No valid clustering solutions found\")\n",
    "\n",
    "optimal_k = k_values[np.argmax(silhouette_scores)]\n",
    "print(f\"\\nOptimal k: {optimal_k}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a921848",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform final clustering using the optimal number of clusters (optimal_k)\n",
    "# Using the same sample as before for consistency in results\n",
    "final_kmedoids = KMedoids(\n",
    "    n_clusters=optimal_k,       # Using the optimal k determined earlier\n",
    "    init='k-medoids++',         # Smart initialization method for better centroids\n",
    "    random_state=42,            # For reproducibility of results\n",
    "    metric='euclidean',         # Using Euclidean distance for spatial data\n",
    "    max_iter=50                 # Maximum iterations for convergence\n",
    ")\n",
    "\n",
    "# Fit the model to our scaled sample data and predict cluster labels\n",
    "final_labels = final_kmedoids.fit_predict(sample_scaled)\n",
    "\n",
    "# Prepare data for visualization by creating a copy of our sample coordinates\n",
    "cluster_viz = sample_coords.copy()\n",
    "# Add cluster labels to our visualization dataframe (converted to string for categorical coloring)\n",
    "cluster_viz['Cluster'] = final_labels.astype(str)\n",
    "# Add the crime category information from the original dataframe\n",
    "cluster_viz['Category'] = train_df.iloc[sample_indices]['Category'].values\n",
    "\n",
    "# Create an interactive scatter plot of the clusters\n",
    "fig = px.scatter(\n",
    "    cluster_viz, \n",
    "    x='X',                     \n",
    "    y='Y',                     \n",
    "    color='Cluster',          \n",
    "    title=f'K-Medoids Clustering (k={optimal_k})', \n",
    "    hover_data=['Category']      # Show crime category when hovering over points\n",
    ")\n",
    "\n",
    "# Adjust marker properties for better visualization\n",
    "fig.update_traces(\n",
    "    marker=dict(\n",
    "        size=4,                 # Smaller points for better visibility in dense areas\n",
    "        opacity=0.7             # Slightly transparent to handle overlapping points\n",
    "    )\n",
    ")\n",
    "\n",
    "# Display the interactive plot\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "792ee133",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through each cluster (from 0 to optimal_k-1)\n",
    "for cluster in range(optimal_k):\n",
    "    # Filter data for the current cluster only\n",
    "    cluster_data = cluster_viz[cluster_viz['Cluster'] == str(cluster)]\n",
    "    \n",
    "    # Create statistics table for basic cluster information\n",
    "    stats_table = [\n",
    "        [\"Number of points\", len(cluster_data)],  # Count of crimes in this cluster\n",
    "        [\"X coordinate range\", f\"{cluster_data['X'].min():.2f} to {cluster_data['X'].max():.2f}\"],  # Longitude range\n",
    "        [\"Y coordinate range\", f\"{cluster_data['Y'].min():.2f} to {cluster_data['Y'].max():.2f}\"],  # Latitude range\n",
    "        [\"Mean coordinates\", f\"X={cluster_data['X'].mean():.2f}, Y={cluster_data['Y'].mean():.2f}\"]  # Cluster center\n",
    "    ]\n",
    "    \n",
    "    # Print cluster header with decorative borders\n",
    "    print(f\"\\n╒{'═'*50}╕\")\n",
    "    print(f\"│ {'Cluster ' + str(cluster):^48} │\")  # Centered cluster title\n",
    "    print(f\"╞{'═'*50}╡\")\n",
    "    # Print the statistics table with grid formatting\n",
    "    print(tabulate(stats_table, tablefmt=\"simple_grid\"))\n",
    "    \n",
    "    # Analyze and display top crime categories in this cluster\n",
    "    top_crimes = cluster_data['Category'].value_counts().head(3).reset_index()\n",
    "    top_crimes.columns = ['Crime Category', 'Count']  # Rename columns\n",
    "    print(\"\\nTop Crime Categories:\")\n",
    "    # Print top crimes with pretty formatting\n",
    "    print(tabulate(top_crimes, headers='keys', tablefmt=\"pretty\", showindex=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc104f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a figure with subplots - one for each cluster\n",
    "# Adjust figure size based on number of clusters (optimal_k)\n",
    "fig, axes = plt.subplots(optimal_k, 1, figsize=(12, 6*optimal_k), dpi=100)\n",
    "\n",
    "# Loop through each cluster to analyze crime distribution\n",
    "for cluster in range(optimal_k):\n",
    "    # Filter data for current cluster\n",
    "    cluster_data = cluster_viz[cluster_viz['Cluster'] == str(cluster)]\n",
    "    \n",
    "    # Get top 20 most frequent crime categories in this cluster\n",
    "    top_crimes = cluster_data['Category'].value_counts().head(20)\n",
    "    \n",
    "    # Create bar plot for this cluster's top crimes\n",
    "    bars = axes[cluster].bar(top_crimes.index, top_crimes.values, \n",
    "                           color=plt.cm.tab20(cluster),  # Different color per cluster\n",
    "                           alpha=0.7)\n",
    "    \n",
    "    # Add value labels on top of each bar\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        axes[cluster].text(bar.get_x() + bar.get_width()/2., height,\n",
    "                         f'{height}',\n",
    "                         ha='center', va='bottom')\n",
    "    \n",
    "    # Customize plot appearance\n",
    "    axes[cluster].set_title(f'Cluster {cluster} - Top Crime Categories (Total: {len(cluster_data)} crimes)',\n",
    "                          fontsize=12, pad=15)\n",
    "    axes[cluster].set_ylabel('Number of Crimes', fontsize=10)\n",
    "    axes[cluster].tick_params(axis='x', rotation=90)\n",
    "    axes[cluster].grid(axis='y', linestyle=':', alpha=0.7)\n",
    "    \n",
    "\n",
    "# Adjust layout to prevent overlapping elements\n",
    "plt.tight_layout(pad=3.0)\n",
    "\n",
    "# Add overall figure title\n",
    "fig.suptitle('Crime Category Distribution Across Clusters', \n",
    "            y=1.02, fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ef48991",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))  # Slightly larger figure for better readability\n",
    "\n",
    "# Create the countplot\n",
    "ax = sns.countplot(x='DayOfWeek', \n",
    "                  data=train_df,\n",
    "                  order=['Monday', 'Tuesday', 'Wednesday', 'Thursday', \n",
    "                         'Friday', 'Saturday', 'Sunday'],\n",
    "                  palette='Reds',\n",
    "                  saturation=0.9)\n",
    "\n",
    "# Customize the plot\n",
    "plt.title('Crime Distribution by Day of Week', fontsize=16, pad=20)\n",
    "plt.xlabel('Day of Week', fontsize=12)\n",
    "plt.ylabel('Number of Crimes', fontsize=12)\n",
    "plt.xticks(rotation=45, ha='right')  # Better angled labels\n",
    "\n",
    "# Add value labels on top of each bar\n",
    "for p in ax.patches:\n",
    "    ax.annotate(f'{p.get_height():,}', \n",
    "                (p.get_x() + p.get_width() / 2., p.get_height()),\n",
    "                ha='center', va='center', \n",
    "                xytext=(0, 5), \n",
    "                textcoords='offset points',\n",
    "                fontsize=10)\n",
    "\n",
    "ax.yaxis.grid(True, linestyle='--', alpha=0.7)\n",
    "ax.set_axisbelow(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55acec1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a figure with custom size \n",
    "plt.figure(figsize=(10, 10))\n",
    "# Create a polar subplot \n",
    "ax = plt.subplot(polar=True)\n",
    "# Extract hour from datetime and store in new column \n",
    "train_df['Hours'] = train_df['Dates'].dt.hour\n",
    "# Create 24 evenly spaced angles for clock hours \n",
    "theta = np.linspace(0, 2*np.pi, 24, endpoint=False)\n",
    "# Count crimes per hour and sort by hour \n",
    "counts = train_df['Hours'].value_counts().sort_index()\n",
    "# Create colored bars for each hour \n",
    "bars = ax.bar(theta, counts, width=0.5, color=plt.cm.Reds(np.linspace(0.2, 0.8, len(counts))))\n",
    "# Set clock orientation (12 at top) \n",
    "ax.set_theta_zero_location('N')\n",
    "# Set clockwise direction \n",
    "ax.set_theta_direction(-1)\n",
    "# Add title with padding \n",
    "plt.title('Crime Clock', pad=20, fontsize=16)\n",
    "# Set hour labels at each angle \n",
    "plt.xticks(theta, range(24))\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18aa446d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a date-only column without time component ---> for cleaner animation\n",
    "# This helps in creating daily animation frames without hourly fluctuations\n",
    "train_df[\"DateOnly\"] = train_df[\"Dates\"].dt.date\n",
    "\n",
    "\"\"\"\n",
    "Interactive Animated Crime Map Visualization\n",
    "Purpose: To visualize the spatial and temporal distribution of crimes in San Francisco\n",
    "\n",
    "Key Features:\n",
    "- Each point represents a crime incident\n",
    "- Color-coded by crime category for easy differentiation\n",
    "- Animated by date to show crime patterns over time\n",
    "- Interactive hover information for detailed crime data\n",
    "- Geographic context with proper map projection\n",
    "\"\"\"\n",
    "\n",
    "# Create the animated scatter plot on mapbox\n",
    "fig = px.scatter_mapbox(\n",
    "    train_df,  # Our crime dataset\n",
    "    lat=\"Y\",  # Latitude values (north-south position)\n",
    "    lon=\"X\",  # Longitude values (east-west position)\n",
    "    color=\"Category\",  # Different colors for each crime type\n",
    "    animation_frame=\"DateOnly\",  # Animate by date (creates time slider)\n",
    "    hover_name=\"Descript\",  # Show crime description on hover\n",
    "    zoom=11,  # Initial zoom level (street-level view)\n",
    "    \n",
    "    # Center the map on downtown San Francisco coordinates\n",
    "    center={\"lat\": 37.76, \"lon\": -122.43},\n",
    "    \n",
    "    height=600,  # Set map height in pixels\n",
    "    mapbox_style=\"carto-positron\",  # Light-themed map for clarity\n",
    "    \n",
    "    # Chart title\n",
    "    title=\"Timeline of Crimes in San Francisco\"\n",
    ")\n",
    "\n",
    "# Display the interactive figure\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b945091",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sunburst chart of crime data \n",
    "fig = px.sunburst(train_df, \n",
    "                  path=['PdDistrict', 'Category', 'Resolution'],  # Hierarchy levels \n",
    "                  maxdepth=2)  # Show only 2 initial levels \n",
    "# Customize chart title \n",
    "fig.update_layout(title='Crime Resolution Flow')  \n",
    "\n",
    "# Display the chart\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
